#!/usr/bin/env python3
"""
ç®€å•çš„OI Wikié¡µé¢å‘ç°è„šæœ¬
"""

import requests
import urllib3
from bs4 import BeautifulSoup
import json
import time
import random

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_page_with_retries(url, max_retries=3):
    """å¸¦é‡è¯•çš„é¡µé¢è·å–"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    session = requests.Session()
    
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•è®¿é—® {url} (ç¬¬{attempt+1}æ¬¡)")
            response = session.get(url, headers=headers, verify=False, timeout=30)
            print(f"çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                return response
            elif response.status_code == 403:
                print("è®¿é—®è¢«æ‹’ç»ï¼Œç­‰å¾…åé‡è¯•...")
                time.sleep(random.uniform(5, 10))
            else:
                print(f"HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(2, 5))
    
    return None

def test_topic_urls():
    """æµ‹è¯•å·²çŸ¥çš„topic URL"""
    test_urls = [
        "topic/rmq/",
        "topic/dsu-app/", 
        "topic/lca/",
        "topic/offline-lca/",
        "topic/tree-centroid/",
        "topic/palindrome/",
        "topic/binary-lifting/",
        "topic/heavy-light/",
        "topic/sqrt-decomposition/",
        "topic/mo-algorithm/",
        
        # å¯èƒ½å­˜åœ¨çš„å…¶ä»–topic
        "topic/dsu/",
        "topic/segment-tree/",
        "topic/fenwick/",
        "topic/trie/",
        "topic/kmp/",
        "topic/hash/",
        "topic/convex-hull/",
        "topic/centroid-decomposition/",
        "topic/link-cut-tree/",
        "topic/persistent/",
    ]
    
    base_url = "https://oi-wiki.org/"
    existing_urls = []
    non_existing_urls = []
    
    print("ğŸ” æµ‹è¯•topic URL...")
    for url_path in test_urls:
        full_url = f"{base_url}{url_path}"
        response = get_page_with_retries(full_url, max_retries=2)
        
        if response and response.status_code == 200:
            # æ£€æŸ¥é¡µé¢å†…å®¹ç¡®è®¤ä¸æ˜¯404é¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('title')
            h1 = soup.find('h1')
            
            # å¦‚æœæœ‰æ­£å¸¸çš„æ ‡é¢˜ä¸”ä¸åŒ…å«"404"ç­‰é”™è¯¯ä¿¡æ¯
            if title and not any(x in title.get_text().lower() for x in ['404', 'not found', 'error']):
                existing_urls.append({
                    'url': url_path,
                    'title': title.get_text().strip() if title else '',
                    'h1': h1.get_text().strip() if h1 else ''
                })
                print(f"âœ… {url_path} - {title.get_text().strip() if title else ''}")
            else:
                non_existing_urls.append(url_path)
                print(f"âŒ {url_path} - é¡µé¢æ— æ•ˆ")
        else:
            non_existing_urls.append(url_path)
            print(f"âŒ {url_path} - è®¿é—®å¤±è´¥")
        
        # éšæœºå»¶æ—¶é¿å…è¢«é™åˆ¶
        time.sleep(random.uniform(1, 3))
    
    return existing_urls, non_existing_urls

if __name__ == "__main__":
    print("ğŸš€ ç®€å•çš„OI Wikié¡µé¢å‘ç°")
    print("=" * 50)
    
    existing, non_existing = test_topic_urls()
    
    print(f"\nğŸ“Š ç»“æœç»Ÿè®¡:")
    print(f"âœ… å­˜åœ¨çš„URL ({len(existing)}):")
    topic_mapping = {}
    for item in existing:
        # ä»æ ‡é¢˜ä¸­æå–ä¸­æ–‡åç§°
        title = item['title']
        h1 = item['h1']
        
        # ä¼˜å…ˆä½¿ç”¨h1ï¼Œç„¶åæ˜¯title
        chinese_name = h1 if h1 and h1 != 'OI Wiki' else title
        if 'OI Wiki' in chinese_name:
            chinese_name = chinese_name.replace('OI Wiki', '').strip(' -|')
        
        topic_mapping[item['url']] = chinese_name
        print(f"  \"{item['url']}\": \"{chinese_name}\",")
    
    print(f"\nâŒ ä¸å­˜åœ¨çš„URL ({len(non_existing)}):")
    for url in non_existing:
        print(f"  {url}")
    
    # ä¿å­˜ç»“æœ
    with open('topic_mapping_corrected.json', 'w', encoding='utf-8') as f:
        json.dump(topic_mapping, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ä¿®æ­£åçš„æ˜ å°„å·²ä¿å­˜åˆ° topic_mapping_corrected.json")


