import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
from pathlib import Path
import time
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import re
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==== Configuration ====
BASE_URL = "https://oi.wiki/"
OUTPUT_DIR = Path("pdf")

# ==== Helper Functions ====

def setup_driver():
    """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨ï¼Œæ”¯æŒPDFæ‰“å°"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    
    # è®¾ç½®æ‰“å°é€‰é¡¹
    chrome_options.add_argument('--enable-print-browser')
    chrome_options.add_argument('--run-all-compositor-stages-before-draw')
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    except Exception as e:
        print(f"âŒ Chrome driver setup failed: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…ChromeDriverå¹¶æ·»åŠ åˆ°PATHä¸­")
        return None

def get_page_title_and_category(soup, url):
    """ä»é¡µé¢ä¸­æå–æ ‡é¢˜å’Œåˆ†ç±»ä¿¡æ¯"""
    # è·å–é¡µé¢æ ‡é¢˜
    title_elem = soup.find('h1')
    if title_elem:
        title = title_elem.get_text().strip()
    else:
        # ä»URLè·¯å¾„æ¨æ–­æ ‡é¢˜
        path = urlparse(url).path.strip('/')
        title = path.split('/')[-1] if path else "index"
    
    # ä»URLè·¯å¾„è·å–åˆ†ç±»
    path_parts = urlparse(url).path.strip('/').split('/')
    if len(path_parts) > 1:
        category = path_parts[0]  # ä½¿ç”¨ç¬¬ä¸€çº§è·¯å¾„ä½œä¸ºåˆ†ç±»
    else:
        category = "å…¶ä»–"
    
    return title, category

def fetch_html(url):
    """è·å–æŒ‡å®šURLçš„HTMLå†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        # ä½¿ç”¨Sessionæ¥ä¿æŒè¿æ¥
        session = requests.Session()
        session.headers.update(headers)
        
        # æ·»åŠ é‡è¯•æœºåˆ¶å’ŒSSLé…ç½®
        resp = session.get(
            url, 
            timeout=(10, 30),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
            verify=False,      # ç¦ç”¨SSLéªŒè¯ä»¥é¿å…è¯ä¹¦é—®é¢˜
            allow_redirects=True,
            stream=False
        )
        resp.raise_for_status()
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = resp.headers.get('content-type', '').lower()
        if 'text/html' not in content_type:
            print(f"âš ï¸ Skipping non-HTML content: {url} (Content-Type: {content_type})")
            return None
            
        return resp.text
        
    except requests.exceptions.SSLError as e:
        print(f"âš ï¸ SSL error for {url}: {e}")
        return None
    except requests.exceptions.Timeout as e:
        print(f"âš ï¸ Timeout error for {url}: {e}")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"âš ï¸ Connection error for {url}: {e}")
        return None
    except requests.exceptions.HTTPError as e:
        code = e.response.status_code if e.response else 'Unknown'
        print(f"âš ï¸ HTTP error {code} for {url}: {e}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Request error for {url}: {e}")
        return None
    except Exception as e:
        print(f"âš ï¸ Unexpected error for {url}: {e}")
        return None

def save_page_as_pdf(driver, url, output_path: Path):
    """å°†ç½‘é¡µä¿å­˜ä¸ºPDF"""
    try:
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
        time.sleep(3)
        
        # ä½¿ç”¨Chromeçš„æ‰“å°åŠŸèƒ½ä¿å­˜ä¸ºPDF
        print_options = {
            'landscape': False,
            'displayHeaderFooter': False,
            'printBackground': True,
            'preferCSSPageSize': True,
        }
        
        result = driver.execute_cdp_cmd('Page.printToPDF', print_options)
        
        # ä¿å­˜PDFæ–‡ä»¶
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'wb') as f:
            import base64
            f.write(base64.b64decode(result['data']))
        
        print(f"âœ… Saved PDF: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ Failed to save PDF for {url}: {e}")
        return False

def is_article_page(url, soup):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ç« é¡µé¢ï¼ˆè€Œéå¯¼èˆªé¡µé¢ï¼‰"""
    # æ’é™¤ä¸»é¡µ
    if url == BASE_URL or url == BASE_URL.rstrip('/'):
        return False
    
    # æ’é™¤æ˜æ˜¾çš„éæ–‡ç« é¡µé¢
    excluded_patterns = [
        '/edit', '/history', '/search', '/special:', 
        '.css', '.js', '.png', '.jpg', '.gif', '.svg',
        '/user:', '/talk:', '/file:', '/category:'
    ]
    
    url_lower = url.lower()
    if any(pattern in url_lower for pattern in excluded_patterns):
        return False
    
    # OI Wikiçš„æ–‡ç« é¡µé¢é€šå¸¸æœ‰ä»¥ä¸‹ç‰¹å¾ä¹‹ä¸€ï¼š
    # 1. æœ‰h1æ ‡é¢˜æ ‡ç­¾
    h1 = soup.find('h1')
    if not h1:
        return False
    
    # 2. æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–‡æœ¬å†…å®¹
    # ä½¿ç”¨å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾ä¸»è¦å†…å®¹
    content_selectors = [
        '.md-content',      # OI Wiki å¯èƒ½ä½¿ç”¨çš„ç±»
        '.content',         # é€šç”¨å†…å®¹ç±»
        'main',            # HTML5 main æ ‡ç­¾
        'article',         # HTML5 article æ ‡ç­¾
        '#content',        # IDä¸ºcontentçš„å…ƒç´ 
        '.wiki-content',   # Wikiå†…å®¹ç±»
        '.page-content'    # é¡µé¢å†…å®¹ç±»
    ]
    
    main_content = None
    for selector in content_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹å®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªbody
    if not main_content:
        main_content = soup.find('body')
    
    if main_content:
        text_content = main_content.get_text().strip()
        # é™ä½æ–‡æœ¬é•¿åº¦è¦æ±‚ï¼Œå› ä¸ºæœ‰äº›æ–‡ç« å¯èƒ½æ¯”è¾ƒçŸ­
        if len(text_content) < 200:  # è‡³å°‘200å­—ç¬¦
            return False
    else:
        return False
    
    # 3. æ£€æŸ¥URLç»“æ„ - OI Wikiçš„æ–‡ç« é€šå¸¸æœ‰å±‚çº§ç»“æ„
    path = urlparse(url).path.strip('/')
    if path and '/' in path:  # æœ‰è·¯å¾„ä¸”æœ‰å±‚çº§ç»“æ„
        return True
    
    # 4. å¦‚æœURLè·¯å¾„ä¸ä¸ºç©ºä¸”ä¸æ˜¯ç‰¹æ®Šé¡µé¢ï¼Œå¯èƒ½æ˜¯æ–‡ç« 
    if path and not any(special in path.lower() for special in ['index', 'main', 'home']):
        return True
    
    return False

# ==== ä¸»è¦æµç¨‹ ====

def crawl_oi_wiki():
    """çˆ¬å–OI Wikiå¹¶ç”ŸæˆPDF"""
    
    # è®¾ç½®æµè§ˆå™¨é©±åŠ¨
    driver = setup_driver()
    if not driver:
        return
    
    try:
        # 1. å‘ç°æ‰€æœ‰æ–‡ç« é¡µé¢
        print("ğŸ” æ­£åœ¨å‘ç°OI Wikiæ–‡ç« é¡µé¢...")
        visited = set()
        
        # ä»ä¸€äº›å·²çŸ¥çš„æ–‡ç« é¡µé¢å¼€å§‹ï¼Œè€Œä¸ä»…ä»…æ˜¯é¦–é¡µ
        initial_urls = {
            BASE_URL,
            "https://oi.wiki/math/number-theory/basic/",
            "https://oi.wiki/ds/stack/",
            "https://oi.wiki/graph/shortest-path/",
            "https://oi.wiki/string/hash/",
            "https://oi.wiki/dp/basic/",
            "https://oi.wiki/search/dfs/",
            "https://oi.wiki/basic/sort/",
            "https://oi.wiki/misc/binary-search/"
        }
        
        to_visit = initial_urls.copy()
        article_pages = []
        
        while to_visit and len(visited) < 200:  # å‡å°‘çˆ¬å–æ•°é‡ï¼Œæé«˜æˆåŠŸç‡
            url = to_visit.pop()
            if url in visited:
                continue
            visited.add(url)
            
            print(f"æ­£åœ¨æ£€æŸ¥: {url}")
            
            html = fetch_html(url)
            if html is None:
                print(f"  âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
                continue
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ç« é¡µé¢
            if is_article_page(url, soup):
                article_pages.append(url)
                print(f"  âœ… å‘ç°æ–‡ç« é¡µé¢")
            else:
                print(f"  â­ï¸ è·³è¿‡éæ–‡ç« é¡µé¢")
            
            # æ”¶é›†æ›´å¤šé“¾æ¥ï¼ˆé™åˆ¶åœ¨åŒåŸŸåä¸‹ï¼‰
            links_added = 0
            for a in soup.find_all('a', href=True):
                if links_added >= 10:  # æ¯é¡µæœ€å¤šæ·»åŠ 10ä¸ªæ–°é“¾æ¥
                    break
                    
                href = urljoin(BASE_URL, a['href'])
                parsed = urlparse(href)
                
                # åªå¤„ç†åŒåŸŸåä¸”æœªè®¿é—®çš„é“¾æ¥
                if (parsed.netloc == urlparse(BASE_URL).netloc and 
                    href not in visited and 
                    len(to_visit) < 100 and  # å‡å°‘å¾…è®¿é—®é˜Ÿåˆ—å¤§å°
                    not any(skip in href.lower() for skip in ['.css', '.js', '.png', '.jpg', 'edit', 'history'])):
                    to_visit.add(href)
                    links_added += 1
            
            # æ·»åŠ å»¶æ—¶é¿å…è¿‡å¿«è¯·æ±‚
            time.sleep(1)
        
        print(f"âœ… å‘ç° {len(article_pages)} ä¸ªæ–‡ç« é¡µé¢")
        
        # 2. ä¸ºæ¯ä¸ªæ–‡ç« é¡µé¢ç”ŸæˆPDF
        print("ğŸ“„ æ­£åœ¨ç”ŸæˆPDFæ–‡ä»¶...")
        success_count = 0
        
        for i, page_url in enumerate(article_pages, 1):
            print(f"å¤„ç†è¿›åº¦: {i}/{len(article_pages)} - {page_url}")
            
            # è·å–é¡µé¢ä¿¡æ¯
            html = fetch_html(page_url)
            if html is None:
                continue
            
            soup = BeautifulSoup(html, 'html.parser')
            title, category = get_page_title_and_category(soup, page_url)
            
            # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
            pdf_filename = f"{safe_title} - OI Wiki.pdf"
            output_path = OUTPUT_DIR / category / pdf_filename
            
            # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
            if output_path.exists():
                print(f"â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_path}")
                continue
            
            # ç”ŸæˆPDF
            if save_page_as_pdf(driver, page_url, output_path):
                success_count += 1
            
            # é€‚å½“å»¶æ—¶é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(1)
        
        print(f"ğŸ‰ å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count} ä¸ªPDFæ–‡ä»¶ï¼Œä¿å­˜åœ¨: {OUTPUT_DIR}")
        
    finally:
        driver.quit()

if __name__ == "__main__":
    crawl_oi_wiki()
